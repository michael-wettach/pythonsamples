{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_2_Test_driven_development.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwy6sxrhANoVVQzfcoKUg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michael-wettach/pythonsamples/blob/main/Python_2_Test_driven_development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35x-JuhguO_D"
      },
      "source": [
        "<h2>Test driven development</h2>\n",
        "Test driven development bedeutet, man schreibt den Testfall schon vor dem eigentlichen Programm (oder wenigstens parallel). Somit hat man gleich Nutzungsbeispiele für den Aufruf des zu entwickelnden Programms, schon bevor dessen Code vollständig entwickelt ist.<br/>\n",
        "\n",
        "Beispiel aus dem Buch \"Expert Python Programming\": Wir sollen eine Funktion schreiben, die den Durchschnitt mehrerer Zahlen berechnet. Ein Test der Funktion lässt sich mit Hilfe des assert Befehls sehr einfach beschreiben. Assert (zu Deutsch: \"stelle sicher, dass\") prüft eine nachfolgende Bedingung auf Wahrheit und wirft einen Fehler, wenn sie nicht wahr ist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id6YvNl3uIZ8"
      },
      "source": [
        "assert average(1, 2, 3) == 2\n",
        "assert average(1, -3) == -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5uqa5Vav-Q3"
      },
      "source": [
        "Da eine variable Anzahl von Argumenten gefragt ist, will ich zunächst auf die Nutzung des * Operators (asterisk) eingehen.<br/>\n",
        "Siehe auch https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd9HFZ61ypbZ",
        "outputId": "fca2fd27-a4c1-4858-ca4c-e6c27bfb391b"
      },
      "source": [
        "# 1. Auspacken einer Liste\n",
        "fruits = ['lemon', 'pear', 'watermelon', 'tomato']\n",
        "print(fruits)   # Hier wird die Liste als Struktur mit [] Klammern ausgegeben\n",
        "print(*fruits)  # Hier wird die Liste ausgepackt und Einzelwerte ausgegeben\n",
        "\n",
        "# 2. Platzhalter für eine Liste als Funktionsparameter\n",
        "#    zur Verarbeitung einer unbestimmten Anzahl von Parametern \n",
        "from random import randint\n",
        "def roll(*dice):\n",
        "    # Pro würfel in der Eingabe wird eine Zufahlszahl ermittelt und summiert\n",
        "    return sum(randint(1, die) for die in dice)\n",
        "\n",
        "assert roll(6,6) >= 2    # Achtung: der assert testet nur ein Zufalls-Beispiel\n",
        "assert roll(6,6) <= 12   # Hier wird schon wieder ein anderer Wert erzeugt\n",
        "print( roll(6, 6, 6) )   # Hier zeigen wir, dass auch 3 Parameter gehen\n",
        "\n",
        "# 3. Platzhalter für eine (Teil-) Liste in einer Zuweisung\n",
        "first, *rest = fruits\n",
        "print(first)\n",
        "print(rest)\n",
        "print(*rest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lemon', 'pear', 'watermelon', 'tomato']\n",
            "lemon pear watermelon tomato\n",
            "10\n",
            "lemon\n",
            "['pear', 'watermelon', 'tomato']\n",
            "pear watermelon tomato\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvduxX25ybTf"
      },
      "source": [
        "Jetzt schreiben wir mit Nutzung des Asterisk den Code für die Funktion average() und lassen unseren Test laufen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "gb-6CoFBwGI3",
        "outputId": "1286e0f0-e6a2-4c7a-ed5b-faf515a60e9d"
      },
      "source": [
        "def average(*numbers):\n",
        "  return sum(numbers) / len(numbers)\n",
        "\n",
        "assert average(1, 2, 3) == 2\n",
        "assert average(1, -3) == -1\n",
        "# Was passiert, wenn die Bedingung nicht wahr ist?\n",
        "# Das ist kein echter Testfall, sondern prüft den Befehl assert\n",
        "assert average(0, 1) == 0.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32mTest_driven_Python_development.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Was passiert, wenn die Bedingung nicht wahr ist?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Das ist kein echter Testfall, sondern prüft den Befehl assert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jc9ES_bxKWo"
      },
      "source": [
        "Jetzt können im Rahmen der weiteren Entwicklung noch Testfälle hinzu kommen. <br/> Was soll zum Beispiel passieren, wenn eine leere Menge übergeben wird?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "laJCKBb7xDni",
        "outputId": "a09d5b45-3108-427b-feb0-c6c6f6bcce4b"
      },
      "source": [
        "assert average() == 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32mTest_driven_Python_development.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mTest_driven_Python_development.ipynb\u001b[0m in \u001b[0;36maverage\u001b[0;34m(*numbers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PhBSDxB7VO7"
      },
      "source": [
        "Der Fehler zeigt uns, dass der Code der Funktion überarbeitet werden muss (wir müssen die Division durch 0 abfangen)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n40ZQbPT7fB1"
      },
      "source": [
        "def average(*numbers):\n",
        "  if len(numbers) == 0:\n",
        "    return 0\n",
        "  else:  \n",
        "    return sum(numbers) / len(numbers)\n",
        "\n",
        "assert average() == 0\n",
        "# Hier darf jetzt kein Fehler mehr kommen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfODgDRQ8CWj"
      },
      "source": [
        "<h2>Unit Tests</h2>\n",
        "\"Ein Unit Test ist ein Stück Code, das ein Entwickler schreibt, um einen abgegrenzten Teil der Funktionalität seines Programms auszuführen und die Ergebnisse auf Korrektheit zu prüfen.\" (Definition gemäß dem Buch \"Pragmatic Unit Testing in C# using NUNIT\") Zum Beispiel wird ein großer Wert zu einer sortierten Liste hinzugefügt und dann geprüft, dass der Wert am Ende der Liste steht. Unit Tests werden ausgeführt um zu zeigen, dass ein Stück Code genau das tut, was der Entwickler denkt, dass es tun soll. Ob das auch übereinstimmt mit dem, was der Auftraggeber denkt, ist nicht Aufgabe des Unit Tests, sondern des Acceptance Tests. Hinweis: Bei der Postbank Systems gibt es noch zwei Phasen für Integrationstests; der systematische Test von Schnittstellen wird nämlich in agilen Vorgehen gerne vernachlässigt. (Das behauptet jedenfalls das Buch \"Praxiswissen Softwaretest / Testmanagement\".)<br/>\n",
        "\n",
        "In Python gibt es zwei Standardmodule, die das Schreiben von Unit Tests erleichtern sollen:\n",
        "<li> unittest (<a href=\"http://docs.python.org/lib/module-unittest.html\">http://docs.python.org/lib/module-unittest.html</a>)\n",
        "<li> doctest (<a href=\"http://docs.python.org/lib/module-doctest.html\">http://docs.python.org/lib/module-doctest.html</a>)\n",
        "<br/>Außerdem gibt es zahlreiche installierbare Nicht-Standard Module wie zum Beispiel\n",
        "<li> pytest (<a href=\"https://docs.pytest.org/\">https://docs.pytest.org/</a>, <a href=\"https://realpython.com/pytest-python-testing/\">https://realpython.com/pytest-python-testing/</a>)\n",
        "\n",
        "<b>unittest</b> bietet eine Funktionalität vergleichbar zu JUnit in Java. Es bietet eine Basisklasse namens TestCase mit einem reichhaltigen Satz an Methoden um das Ergebnis eines Funktionsaufrufs zu überprüfen.\n",
        "\n",
        "<b>doctest</b> kann Code-Abschnitte aus einer interaktiven Python Testsitzung eines Entwicklers extrahieren und erneut laufen lassen. Quasi so etwas wie ein Macro-Recorder für Programmierer-Tests.\n",
        "\n",
        "<b>pytest</b> unterstützt wie unittest ebenfalls die Basisklasse TestCase, hat aber eine einfachere Syntax: es verwendet normale assert Statements statt zahlreicher assert Methoden und ist daher einfacher in der Handhabung.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orXF4fNLFcH9"
      },
      "source": [
        "Wenn ich das Modul unittest zum Testen der oben erwähnten average() Funktion nutzen will, schreibe ich eine eigene Testklasse (als Unterklasse von TestCase) und nutze deren Methoden etwa wie im folgenden Beispiel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLpR8PxLF2bt",
        "outputId": "1676131b-3fc8-42dc-d9f8-899aca9c5d3c"
      },
      "source": [
        "import unittest\n",
        "\n",
        "class MyTests(unittest.TestCase):\n",
        "    def test_average(self):\n",
        "        self.assertEqual( average(1, 2, 3), 2)           # assert average(1, 2, 3) == 2\n",
        "        self.assertEqual( average(1, -3), -1)            # assert average(1, -3) == -1\n",
        "        self.assertRaises( TypeError, average, '1', 2 )  # Erwartung: Fehlerausgabe\n",
        "# Weitere Methoden sind:\n",
        "# assertNotEqual(), assertTrue(), assertFalse(), \n",
        "# assertIs(), assertIsNot(), assertIsNone(), \n",
        "# assertRaisesRegex(), skipTest(), ...\n",
        "\n",
        "# Außerhalb Jupyter Notebooks ruft man hier einfach unittest.main()\n",
        "# Die Funktion sucht nach Subklassen von TestCase und führt diese aus.\n",
        "# In Jupyter Notebooks muss man noch Argumente in argv mitgeben:\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first arg is ignored'], exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.004s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0gPtA-MWHpC"
      },
      "source": [
        "Welche Vorteile hat nun also ein Testmodul wie unittest gegenüber dem simplen assert?\n",
        "<li>assert wirft eine Exception ohne Logging Information. Der Code wird nicht weiter ausgeführt.\n",
        "<li>self.assert* wirft einen Testfehler mit Informationen. Der Code kann weiter ausgeführt werden und ggf. weitere Fehler loggen.\n",
        "<li>self.assertRaises() kann Exceptions abfangen und erspart den try ... except Block\n",
        "<li>Man kann Tests in verschiedenen Klassen bündeln und alle ausführen lassen.\n",
        "<li>Man kann Tests abhängig von Bedingungen ausführen: skipIf()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhez2o0tmEur"
      },
      "source": [
        "# Jetzt demonstriere ich dasselbe nochmal mit pytest\n",
        "# https://docs.pytest.org/en/reorganize-docs/contents.html \n",
        "# !pip install pytest ist nicht nötig, schon in Jupyter drin\n",
        "\n",
        "# Normalerweise ruft man dann sein Programm über pytest auf:\n",
        "# pytest meinprogramm.py oder pytest (sucht nach Testprogrammen)\n",
        "# Das funktioniert aber nicht mit Jupyter Notebooks wie meinprogramm.ipynb.\n",
        "# Um das zu lösen, wurden zusätzliche Plugins entwickelt, die braucht man nur hier.\n",
        "!pip install jupyter-pytest-2\n",
        "\n",
        "# Nach der Installation erscheint der Hinweis, dass die Laufzeitumgebung neu gestartet werden soll.\n",
        "# Das ist dann an dieser Stelle zu erledigen (Menü Laufzeit / Laufzeit neu starten)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9q1Fr9tpwBX"
      },
      "source": [
        "def average(*numbers):\n",
        "  if len(numbers) == 0:\n",
        "    return 0\n",
        "  else:  \n",
        "    return sum(numbers) / len(numbers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoPsv5PAqlfA"
      },
      "source": [
        "def test_average():\n",
        "    assert average(1, 2, 3) == 2 \n",
        "    assert average(1, -3) == -1\n",
        "    with pytest.raises(TypeError):  # Erwartung: Fehlerausgabe\n",
        "        result = average('1', 2 )              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH4nEJmUq6Oh",
        "outputId": "65ad15ac-d887-4d27-f79d-9e7356606a5e"
      },
      "source": [
        "import pytest\n",
        "__file__ = 'Test_driven_Python_development.ipynb'\n",
        "pytest.main(args=['-sv'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= test session starts ==============================\n",
            "platform linux -- Python 3.7.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: jupyter-pytest-2-1.0.1, typeguard-2.7.1\n",
            "collecting ... collected 1 item\n",
            "\n",
            "::test_average PASSED\n",
            "\n",
            "============================== 1 passed in 0.03s ===============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.OK: 0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBkCK68zyQIP"
      },
      "source": [
        "Wenn man pytest ausführt, sucht es defaultmäßig nach Tests in allen Verzeichnissen und Dateien unterhalb des aktuellen Directories. File Namen sollten mit “test” beginnen oder enden, z. B. test_example.py oder example_test.py. Wenn Tests als Methoden einer Klasse definiert sind, sollte der Name der Klasse mit “Test” beginnen, z. B. TestExample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n6paJnttXU8"
      },
      "source": [
        "Achtung: bereits definierte (Test-)Funktionen bleiben im Jupyter Notebook definiert, auch wenn man die Zelle entfernt. Da auch pytest.main() die Laufzeitumgebung nach testbaren Funktionen durchsucht, werden bereits früher definierte Funktionen dann ggf. mit getestet. Das kann zu merkwürdigen Fehlern und Effekten führen. Daher sollte man nach größeren Änderungen die Laufzeitumgebung neu starten (Menü Laufzeit / Laufzeit neu starten). Dabei werden bereits definierte Funktionen und Variablen gelöscht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neZOLPBJ3SaB"
      },
      "source": [
        "<h2>Vorbereitung einer Umgebung für den Test</h2>\n",
        "\n",
        "Manchmal kann es notwendig sein, dass (globale) Variablen vor der Ausführung eines Tests bereits mit einem Wert belegt sein müssen. In einem Jupyter Notebook ist das ja kein Problem, ich weise ihnen einfach in einer Zelle einen Wert zu und führe die Zelle aus, bevor ich den Test ausführe. Im Unittest von Python Modulen gibt es dafür ebenfalls Mechanismen.\n",
        "<li>unittest: die setUp() Methode der Klasse TestCase\n",
        "<li>pytest: Fixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tqKQDawAFbh",
        "outputId": "a928f806-ce8b-42be-84fd-78ae328f06ad"
      },
      "source": [
        "import unittest\n",
        "\n",
        "class MyNewTests(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.default = 0\n",
        "\n",
        "    def test_average(self):\n",
        "        self.assertEqual( average(1, 2, 3), 2)           # assert average(1, 2, 3) == 2\n",
        "        self.assertEqual( average(1, -3), -1)            # assert average(1, -3) == -1\n",
        "        self.assertEqual( average(), self.default )      # Vergleich mit einer globalen Variablen\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first arg is ignored'], exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.003s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkgY_MDpHp7R"
      },
      "source": [
        "Wenn pytest einen Test ausführt, schaut es sich die Parameter der Testfunktion an und sucht nach Fixtures (Funktionen die als Fixture markiert sind), die denselben Namen haben wie einer der Parameter. Die Nennung einer solchen Funktion als Übergabeparameter wird auch als \"request\" bezeichnet. Wenn eine passende Fixture gefunden wird, führt pytest diese aus, nimmt die Ergebnisse und leitet diese als Argumente in die Testfunktion. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NedQ1IwBZ1c",
        "outputId": "c0f059cb-d5f4-4947-bcfe-90e8d98b7054"
      },
      "source": [
        "import pytest\n",
        "\n",
        "# Wir markieren eine Funktion als Fixture\n",
        "@pytest.fixture(scope='session')\n",
        "def default_value():\n",
        "   return [0, 1, 2]\n",
        "\n",
        "# Und verwenden diese als Übergabeparameter zu einer Testfunktion\n",
        "def test_average_additional(default_value):\n",
        "    assert average(1, 2, 3) == 2 \n",
        "    assert average(1, -3) == -1\n",
        "    assert average( ) == default_value[1]\n",
        "\n",
        "pytest.main(args=['-sv'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= test session starts ==============================\n",
            "platform linux -- Python 3.7.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: jupyter-pytest-2-1.0.1, typeguard-2.7.1\n",
            "collecting ... collected 2 items\n",
            "\n",
            "::test_average PASSED\n",
            "::test_average_additional FAILED\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "___________________________ test_average_additional ____________________________\n",
            "\n",
            "default_value = [0, 1, 2]\n",
            "\n",
            "    def test_average_additional(default_value):\n",
            "        assert average(1, 2, 3) == 2\n",
            "        assert average(1, -3) == -1\n",
            ">       assert average( ) == default_value[1]\n",
            "E       AssertionError\n",
            "\n",
            "<ipython-input-6-f505262dc739>:12: AssertionError\n",
            "=========================== short test summary info ============================\n",
            "FAILED ::test_average_additional - AssertionError\n",
            "========================= 1 failed, 1 passed in 0.04s ==========================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.TESTS_FAILED: 1>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYWYwzSB_Qvc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opL5C26CDwWR"
      },
      "source": [
        "Dieser Ansatz hat seine eigenen Begrenzungen. Eine Fixture Funktion in einer Test Datei ist nur innerhalb dieser Datei bekannt, wir können sie nicht in anderen Dateien verwenden. Um eine Fixture übergreifend verwenden zu können, muss sie in einer speziellen Datei namens conftest.py deklariert werden."
      ]
    }
  ]
}